"""
Test gradient flow to ensure no information leakage from future tokens.
This script verifies:
1. Second forward pass with no_grad + detach prevents gradient flow
2. aux_loss only updates backbone through causal (first) forward pass
3. Future summaries don't leak gradient information
"""

import torch
import torch.nn as nn


class SimpleModel(nn.Module):
    """Simple model for testing gradient flow."""
    def __init__(self, hidden_size=16):
        super().__init__()
        self.embedding = nn.Embedding(10, hidden_size)
        self.layer1 = nn.Linear(hidden_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, hidden_size)

    def forward(self, input_ids, output_hidden_states=False):
        x = self.embedding(input_ids)
        h1 = torch.relu(self.layer1(x))
        h2 = torch.relu(self.layer2(h1))
        if output_hidden_states:
            return type('obj', (object,), {'hidden_states': (x, h1, h2)})()
        return h2


class SimpleFuturePredictor(nn.Module):
    """Simple future predictor for testing."""
    def __init__(self, hidden_size=16):
        super().__init__()
        self.proj = nn.Linear(hidden_size, hidden_size)

    def forward(self, x):
        return self.proj(x)


class SimpleMIEstimator(nn.Module):
    """Simple MI estimator for testing."""
    def __init__(self):
        super().__init__()
        self.temperature = 0.1

    def forward(self, predicted, target):
        # Simple MSE loss for testing
        return ((predicted - target) ** 2).mean()


def test_no_grad_detach_prevents_gradient_flow():
    """Test that no_grad + detach prevents gradient from flowing to second forward."""
    print("\n" + "="*80)
    print("TEST 1: no_grad + detach Prevents Gradient Flow")
    print("="*80)

    torch.manual_seed(42)
    model = SimpleModel(hidden_size=16)

    # Create input
    input_ids = torch.tensor([[1, 2, 3, 4]])

    # First forward (causal) - with grad
    print("\n[Step 1] First forward pass (causal, with grad)")
    output1 = model(input_ids, output_hidden_states=True)
    hidden_states1 = output1.hidden_states[-1]  # (1, 4, 16)

    assert hidden_states1.requires_grad, "First forward should track gradients"
    print(f"  hidden_states1.requires_grad: {hidden_states1.requires_grad} ‚úì")

    # Second forward (anti-causal) - with no_grad + detach
    print("\n[Step 2] Second forward pass (anti-causal, with no_grad + detach)")
    with torch.no_grad():
        output2 = model(input_ids, output_hidden_states=True)
        hidden_states2 = output2.hidden_states[-1].detach()

    assert not hidden_states2.requires_grad, "Second forward should NOT track gradients"
    print(f"  hidden_states2.requires_grad: {hidden_states2.requires_grad} ‚úì")

    # Compute loss using both
    print("\n[Step 3] Compute loss and check gradient flow")
    loss = ((hidden_states1 - hidden_states2) ** 2).mean()

    # Record initial parameters
    initial_params = {name: param.clone() for name, param in model.named_parameters()}

    # Backward
    loss.backward()

    # Check that gradients exist for all model parameters
    for name, param in model.named_parameters():
        assert param.grad is not None, f"Parameter {name} should have gradients"

    print("  ‚úÖ All model parameters received gradients from first forward")

    # The key test: verify that hidden_states2 doesn't contribute to gradient
    # We can test this by checking that the gradient flows only through hidden_states1
    print("\n[Step 4] Verify gradient isolation")

    # Create a separate test where we only use hidden_states2
    model.zero_grad()
    with torch.no_grad():
        output_isolated = model(input_ids, output_hidden_states=True)
        hidden_isolated = output_isolated.hidden_states[-1].detach()

    # Try to compute loss and backward
    loss_isolated = (hidden_isolated ** 2).mean()

    # This should not create gradients (because everything is detached)
    try:
        loss_isolated.backward()
        gradients_exist = any(p.grad is not None for p in model.parameters())
        assert not gradients_exist, "Detached tensors should not create gradients"
    except RuntimeError:
        # Expected: backward on a tensor that doesn't require grad
        pass

    print("  ‚úÖ Detached hidden states do not create gradients")
    print("\n‚úÖ TEST PASSED: no_grad + detach correctly prevents gradient flow!")


def test_aux_loss_gradient_flow():
    """Test that aux_loss updates backbone, future_predictor, and mi_estimator correctly."""
    print("\n" + "="*80)
    print("TEST 2: aux_loss Gradient Flow Through Components")
    print("="*80)

    torch.manual_seed(42)

    # Create components
    model = SimpleModel(hidden_size=16)
    future_predictor = SimpleFuturePredictor(hidden_size=16)
    mi_estimator = SimpleMIEstimator()

    input_ids = torch.tensor([[1, 2, 3, 4]])

    # Simulate the training loop
    print("\n[Step 1] First forward (causal)")
    output = model(input_ids, output_hidden_states=True)
    hidden_states = output.hidden_states[-1]  # With gradient

    print(f"  hidden_states.requires_grad: {hidden_states.requires_grad}")

    print("\n[Step 2] Second forward (anti-causal, no_grad + detach)")
    with torch.no_grad():
        future_output = model(input_ids, output_hidden_states=True)
        future_summaries_detached = future_output.hidden_states[-1].detach()

    print(f"  future_summaries_detached.requires_grad: {future_summaries_detached.requires_grad}")

    print("\n[Step 3] Future predictor forward")
    predicted_future = future_predictor(hidden_states)
    print(f"  predicted_future.requires_grad: {predicted_future.requires_grad}")

    print("\n[Step 4] Compute aux_loss")
    aux_loss = mi_estimator(predicted_future, future_summaries_detached)
    print(f"  aux_loss: {aux_loss.item():.4f}")
    print(f"  aux_loss.requires_grad: {aux_loss.requires_grad}")

    # Record parameter counts before backward
    print("\n[Step 5] Backward pass")
    aux_loss.backward()

    # Check gradients
    print("\n[Step 6] Verify gradient flow")

    # Model (backbone) should have gradients
    model_has_grads = all(p.grad is not None for p in model.parameters() if p.requires_grad)
    print(f"  Backbone (model) has gradients: {model_has_grads}")
    assert model_has_grads, "Backbone should receive gradients through hidden_states"

    # Future predictor should have gradients
    fp_has_grads = all(p.grad is not None for p in future_predictor.parameters() if p.requires_grad)
    print(f"  Future predictor has gradients: {fp_has_grads}")
    assert fp_has_grads, "Future predictor should receive gradients"

    # MI estimator should have gradients (if it has parameters)
    mi_params = list(mi_estimator.parameters())
    if len(mi_params) > 0:
        mi_has_grads = all(p.grad is not None for p in mi_params if p.requires_grad)
        print(f"  MI estimator has gradients: {mi_has_grads}")
    else:
        print(f"  MI estimator has no parameters (expected for simple MSE)")

    print("\n  ‚úÖ All components receive gradients correctly")

    # Verify that gradients came from the FIRST forward (causal), not the second
    print("\n[Step 7] Verify gradient source")

    # Create a test where we ONLY use second forward (detached input)
    model2 = SimpleModel(hidden_size=16)
    future_predictor2 = SimpleFuturePredictor(hidden_size=16)

    with torch.no_grad():
        output_test = model2(input_ids, output_hidden_states=True)
        hidden_test = output_test.hidden_states[-1].detach()

    # predicted_test will still require_grad because future_predictor has parameters
    # But gradients won't flow back to model2
    predicted_test = future_predictor2(hidden_test)
    loss_test = (predicted_test ** 2).mean()
    loss_test.backward()

    # Check: model2 should NOT have gradients (because hidden_test was detached)
    model2_has_grads = any(
        p.grad is not None and p.grad.abs().sum() > 0
        for p in model2.parameters()
    )
    assert not model2_has_grads, \
        "Model2 should not receive gradients when input is detached"

    # But future_predictor2 SHOULD have gradients (its own parameters)
    fp2_has_grads = all(p.grad is not None for p in future_predictor2.parameters())
    assert fp2_has_grads, "Future predictor should still receive gradients from its parameters"

    print("  ‚úÖ Gradients cannot flow back through detached hidden states")
    print("  ‚úÖ But gradients still flow to predictor's own parameters")

    print("\n‚úÖ TEST PASSED: aux_loss correctly updates all components through causal path!")


def test_information_leakage_prevention():
    """Test that future information doesn't leak into current predictions."""
    print("\n" + "="*80)
    print("TEST 3: Information Leakage Prevention")
    print("="*80)

    torch.manual_seed(42)

    model = SimpleModel(hidden_size=16)
    future_predictor = SimpleFuturePredictor(hidden_size=16)

    input_ids = torch.tensor([[1, 2, 3, 4]])

    print("\n[Scenario 1] Training with aux_loss (correct way)")
    print("-" * 80)

    # Save initial model state
    initial_state = {name: param.clone() for name, param in model.named_parameters()}

    optimizer = torch.optim.SGD(
        list(model.parameters()) + list(future_predictor.parameters()),
        lr=0.01
    )

    # Training step
    optimizer.zero_grad()

    # First forward (causal) - model sees only past
    output1 = model(input_ids, output_hidden_states=True)
    hidden1 = output1.hidden_states[-1]

    # Second forward (anti-causal) - model sees future, but detached
    with torch.no_grad():
        output2 = model(input_ids, output_hidden_states=True)
        hidden2 = output2.hidden_states[-1].detach()

    # Compute aux loss
    predicted = future_predictor(hidden1)
    aux_loss = ((predicted - hidden2) ** 2).mean()

    print(f"  aux_loss: {aux_loss.item():.6f}")

    # Backward and optimize
    aux_loss.backward()

    # Check that model parameters received gradients
    for name, param in model.named_parameters():
        assert param.grad is not None, f"Model parameter {name} should have gradient"
        grad_norm = param.grad.norm().item()
        print(f"    {name} gradient norm: {grad_norm:.6f}")

    optimizer.step()

    # Verify model parameters changed
    for name, param in model.named_parameters():
        initial = initial_state[name]
        changed = not torch.allclose(param, initial)
        print(f"    {name} changed: {changed}")
        assert changed, f"Parameter {name} should have been updated"

    print("  ‚úÖ Model updated through causal forward path")

    print("\n[Scenario 2] Verify detached target doesn't propagate gradients to its source")
    print("-" * 80)

    # New test: verify that using detached target doesn't create gradients in source model
    model_source = SimpleModel(hidden_size=16)
    model_target = SimpleModel(hidden_size=16)
    predictor = SimpleFuturePredictor(hidden_size=16)

    # Source model generates target (this will be detached)
    with torch.no_grad():
        source_output = model_source(input_ids, output_hidden_states=True)
        target = source_output.hidden_states[-1].detach()

    # Target model generates prediction
    target_output = model_target(input_ids, output_hidden_states=True)
    hidden = target_output.hidden_states[-1]
    predicted = predictor(hidden)

    # Compute loss
    loss = ((predicted - target) ** 2).mean()
    loss.backward()

    # Verify: source_model should NOT have gradients
    source_has_grads = any(
        p.grad is not None and p.grad.abs().sum() > 0
        for p in model_source.parameters()
    )
    print(f"  Source model has gradients: {source_has_grads}")
    assert not source_has_grads, "Source model should not receive gradients (target is detached)"

    # Verify: target_model SHOULD have gradients
    target_has_grads = all(
        p.grad is not None for p in model_target.parameters() if p.requires_grad
    )
    print(f"  Target model has gradients: {target_has_grads}")
    assert target_has_grads, "Target model should receive gradients"

    print("  ‚úÖ Detached target isolates source model from gradient flow")

    print("\n[Scenario 3] Verify gradient flow path")
    print("-" * 80)

    # Reset for clarity
    model3 = SimpleModel(hidden_size=16)
    predictor3 = SimpleFuturePredictor(hidden_size=16)

    # Create a fixed random target (simulating detached future summaries)
    torch.manual_seed(100)
    fixed_target = torch.randn(1, 4, 16)  # Not connected to any model

    # Forward through model
    output3 = model3(input_ids, output_hidden_states=True)
    hidden3 = output3.hidden_states[-1]
    predicted3 = predictor3(hidden3)

    # Loss
    loss3 = ((predicted3 - fixed_target) ** 2).mean()
    loss3.backward()

    # Verify gradients
    print(f"  Model3 has gradients: {all(p.grad is not None for p in model3.parameters())}")
    print(f"  Predictor3 has gradients: {all(p.grad is not None for p in predictor3.parameters())}")

    print("  ‚úÖ Gradient flows correctly through causal path to fixed target")

    print("\n‚úÖ TEST PASSED: No information leakage from future tokens!")


def run_all_tests():
    """Run all gradient flow tests."""
    print("\n")
    print("‚ïî" + "="*78 + "‚ïó")
    print("‚ïë" + " "*18 + "GRADIENT FLOW TEST SUITE" + " "*34 + "‚ïë")
    print("‚ïö" + "="*78 + "‚ïù")

    try:
        test_no_grad_detach_prevents_gradient_flow()
        test_aux_loss_gradient_flow()
        test_information_leakage_prevention()

        print("\n" + "="*80)
        print("üéâ ALL GRADIENT FLOW TESTS PASSED! üéâ")
        print("="*80)
        print("\nSummary:")
        print("  ‚úÖ no_grad + detach correctly prevents gradient flow")
        print("  ‚úÖ aux_loss updates backbone through causal (first) forward only")
        print("  ‚úÖ aux_loss updates future_predictor and mi_estimator")
        print("  ‚úÖ Second forward (anti-causal) is properly isolated")
        print("  ‚úÖ No information leakage from future tokens")
        print("\nConclusion:")
        print("  The implementation correctly prevents future information from")
        print("  influencing the backbone model through gradient updates.")
        print("  The model learns to predict future representations without")
        print("  ever seeing them during its own forward pass.")
        print()

    except AssertionError as e:
        print("\n" + "="*80)
        print("‚ùå TEST FAILED!")
        print("="*80)
        print(f"Error: {e}")
        raise
    except Exception as e:
        print("\n" + "="*80)
        print("‚ùå UNEXPECTED ERROR!")
        print("="*80)
        print(f"Error: {e}")
        raise


if __name__ == "__main__":
    run_all_tests()
